---
title: "Regression LASSO"
categories: ML
tags: [ML]
toc: true
math: true
---

## Introduction


Après avoir étudier Ridge en profondeur en va s'occuper de son alter ego: Lasso ou **Least absolute shrinkage and selection operator**. On a vu que Ridge tirait les coefficients vers 0, mais ne les annulaient pas. Lasso, lui permet d'éliminer des variables ou autrement dit de sélectionner les variables pour ne garder que des variables signifcatives.
Lasso est un modèle de régularisation et certains coefficients vaudront donc exactement 0.

## LASSO

### Modèle

Au lieu d'avoir une régularisation L2 comme dans Ridge, LASSO propose une régularisation L1:

$$
\hat{\beta}_{\text{lasso}}
=
\arg\min_{\beta}
\left(
  \frac{\| y - X\beta \|_2^2}{2}
+
\lambda\| \beta \|_1
\right)
$$ 
où $X \in \mathbb{R}^{n\times p}$ et $\beta \in \mathbb{R}^{p}$


Cette fois on ne pourra pas résoudre ce problème avec une forme explicite comme pour Ridge ou OSL ou du moins pas directement.
On peut ajouter un facteur $\frac{1}{n}$ que l'on a aussi omis dans le problème ridge qui permet d'avoir un $\lambda$ indépendant de la taille du dataset ce qui permet des comparaisons.
Ici néanmoins, on ne le met pas afin d'avoir une expression simple et on ne compte pas faire des comparaisons entre dataset. Cela signifie que avec $\lambda$ varie en fonction de la taille du dataset.
### Approche de résolution

Plusieurs algorithmes plus ou moins performants existent, mais nous choisissons ici d'utiliser le coordinate descent normale.
Scikit utilise le coordinate decent safe screening gap qui utilise des concepts de convexité, dualité et autres ce qui va trop loin pour le projet ici présent.

Ici nous allons donc utiliser le coordinate descent tout court. L'idée c'est que même si le LASSO n'admet pas de solution explicite, si on se place en 1 dimension, on a une solution. 
On va donc optimiser non pas globalement, mais par variable et itérer sur les variables.

Commençons par dériver la solution explicite pour une coordonnée $\beta_{j}$ où $j \in \{1,\dots,p\}$. On remarque le problème se réécrit alors 
$$
\hat{\beta}_{j}
=
\arg\min_{\beta_{j}}
\left(
  \frac{\| y - X_{-j}\beta_{-j} - \beta_{j}x_{j} \|_2^2}{2}
+
\lambda |\beta_{j}|
\right)
=\arg\min_{\beta_{j}}(\rho(\beta_j))
$$ 
Avec $X=(x_1,\dots,x_p)$, $\quad X_{-j}=(x_1,\dots,x_{j-1},0,x_{j+1},\dots,x_p)$ et $\beta_{-j}$ le vecteur $\beta$ privé de $\beta_j$. On note donc que $X_{-j}\beta_{-j}$ est **une somme de vecteurs** et en notant $r_{j}=y - X_{-j}\beta_{-j}$, $\quad \mathbf{r_j}$ **est indépendant de** $\mathbf{\beta_j}$.

Pour être bien au clair sur les notations, on a $x_j= (X_{1,j},\dots,X_{n,j})$

La valeur absolue n'étant pas différentiable en 0, on utilise la notion de sous gradient pour calculer le gradient de $\rho$, puis la condition de fermat pour trouver la formule explicite de $\beta_j$. 
$$
\partial |x| =
\begin{cases}
\{-1\} & \text{si } x < 0 \\
\{1\}  & \text{si } x > 0 \\
[-1,1] & \text{si } x = 0
\end{cases}
$$

C'est cet intervalle $[-1,1]$ qui permet aux coefficients de valoir strictement 0 pour certaines valeurs de $\lambda$ ce que l'on va comprendre dans un instant.
En dérivant donc $\rho$, on obtient 
$$
\partial \rho(\beta_j) =
\begin{cases}
\{-\lambda+\beta_j \|x_j\|_2^2-x_{j}^\top r_{j}\} & \text{si } \beta_j < 0 \\
\{\lambda +\beta_j \|x_j\|_2^2-x_{j}^\top r_{j}\}  & \text{si } \beta_j > 0 \\
[-\lambda-x_{j}^\top r_{j},\lambda-x_{j}^\top r_{j}] & \text{si } \beta_j = 0
\end{cases}
$$

On utilise enfin la condition de fermat pour conclure que:

$$
\beta_j \|x_j\|_2^2 =
\begin{cases}
\lambda+x_{j}^\top r_{j} & \text{si } x_{j}^\top r_{j} < -\lambda \\
-\lambda+x_{j}^\top r_{j} & \text{si } x_{j}^\top r_{j} >\lambda \\
0 & \text{si } \lambda \ge |x_{j}^\top r_{j}|
\end{cases}
$$

Le troisième cas est celui qui donne des conditions pour avoir un coefficient valant purement 0 en fonction de la valeur de $\lambda$ par rapport aux données.
En divisant par $\|x_j\|_2^2$ et en compactant l'expression, on obtient:
$$
\beta_j=\operatorname{sign}(x_{j}^\top r_{j})\frac{\max(0,|x_{j}^\top r_{j}|-\lambda)}{\|x_j\|_2^2}
$$

Il ne faut pas oublier que le résidu $r_j$ dépend des coefficients $\beta_{-j}$.
Par conséquent, la mise à jour de $\beta_j$ s’effectue en considérant les autres
coefficients comme fixés. L’algorithme de coordinate descent consiste donc
en des passes successives sur les coordonnées, chaque coefficient étant mis
à jour à tour de rôle.

 On utilise ici un critère de convergence simple basé sur un nombre maximal d’itérations, correspondant à un nombre fixé de passes sur
l’ensemble des $p$ coefficients (par exemple $m$ passes). Bien que la convergence
du coordinate descent soit garantie pour le problème du LASSO, l’utilisation
de critères plus fins tels que les conditions KKT ou les méthodes de
safe screening gap n’est pas retenue ici.

### Résolution numérique


---
title: "LMM: Linear Mixed Models"
categories: ML
tags: [ML, linear]
toc: true
math: true
---

## Introduction

Les modèles linéaire à effets mixtes (LMM) **servent d'abord à représenter la structure de données non-indépendantes (par exemple des mesures répétées de patients en santé) et expliquer ce qui se passe. La prédiction est secondaire. ** 




### p-value et t-value

La p-value, par définition, c'est la probabilité d'avoir une valeur aussi extrème que celle observée. Avec une phrase bien moins compliqué, la p-value compare les données et en particulier ce qu'on observe avec l'hypothèse H0 pour voir si celle-ci est valide sur les données.

Si on suppose (H0) que vivre éxagérement proche d'une centrale nucléaire n'implique pas uen ahusse d taux de cancer (on va le supposer la probabilité d'avoir un cancer de 0.005, sur 5 ans) et qu'on observe 12 personnes sur 1000 qui ont en cancer sur 5 ans en vivant à moins de 500m de la centrale. Avec une simulation monte-carlo, on trouve une p-value de environ 0.005, par conséquent notre observation plaide fortement contre l'hypothèse H0 et que celle-ci explique mal les données.
**On ne conclut pas que la centrale cause plus de cancer ou que ce n'est pas du hasard, mais uniquement que le modèle est peu compatible avec les données.**
Je renvoie à ce [lien](https://bookdown.org/jgscott/DSGI/p-values.html) qui permet de vraiment mieux appréhender les p-values.

Maintenant les p-values ne vont pas être possibles à calculer en LMM ou vont être approximatives.
C'est pour quoi on s'intérèsse à l'infos brute: la statistique nommée t-value définie comme $\frac{\beta}{SE(\beta)}$ où $\beta$ est un paramètre de la régréssion. En LMM, on l'utilise car on suppose notre estimateur $\beta$ suivant asymptotiquement une loi normale et que sa variance est estimée ou connue. Son nom change dans le cadre théorique d'une LMM, car la loi de la t-value n'est pas connue, on l'appelle alors une statistique de Wald.

La statistique de Wald compare l’estimation $\beta$ à 0 en la rapportant à son erreur standard, c’est-à-dire à l’incertitude sur cette estimation. Elle indique combien d’écarts-types séparent $\beta$ de la valeur nulle.

La t-value est plus difficile à interpréter, car elle nécéssite du contexte pour être interpréter (2 mêmes valeurs de t-values peuvent impliquer des p-values totalement différentes). Elle est néanmoins plus informative que la p-value, celle-ci étant une traduction de la t-value en écrasant de d'information (par exemple la taille des données). Une p-value peut en effet être construite à partir de la t-value et du nombre de degré de liberté du modèle. Le soucis est que en LMM on ne connait pas la loi exacte de la statistique (t-value) sous H0, car on a des observations corrélées, des degrés de liberté flous et des variances aléatoires estimées. 

La t-value donne aussi le signe de $\beta$ (pour la relation entre variable prédictive et variable réponse).


En cours de construction
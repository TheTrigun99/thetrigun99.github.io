---
title: "Regression Ridge"
categories: ML
tags: [ML]
toc: true
math: true
---

## Introduction

Commençons au commencement: **la régréssion OLS (ordinary least square)** qui est la régréssion linéaire de base et aussi le classique des cours de statistiques.
Pour rappel, la formule de close de l'estimateur est $\hat{\beta} = (X^{\top} X)^{-1} X^{\top} y$ 

$ X \in \mathbb{R}^{n \times p}$ est la matrice des variables explicatives (matrice de design), $y \in \mathbb{R}^{n}$ est le vecteur des observations, $\hat{\beta} \in \mathbb{R}^{p}$ est l’estimateur des paramètres du modèle, $n$ est le nombre d’observations,$p$ est le nombre de paramètres (variables explicatives, **intercept inclus**).

On rappelle les hypothèses:

$$
y = X\beta + \varepsilon,
\qquad
\varepsilon \sim \mathcal{N}(0, \sigma^2 I_n).
$$
Ce qui est intéréssant pour y voir plus clair dans les soucis que pose la OSL, c'est la décomposition en valeurs singulières (SVD). Avec celle-ci, on a directement que 

$$
\operatorname{Var}\!\left(\hat{\beta}_{\mathrm{OLS}}\right)
= \sigma^{2} V \operatorname{diag}\!\left(\frac{1}{\lambda_1},\dots,\frac{1}{\lambda_i}\right) V^{\top}
$$

où $\lambda_i = \alpha_{i}^2$ avec $\alpha_i$ les valeurs singulières de $X$. 
et en se plaçant dans la base propre, on s'intéresse aux coefficient $ \frac{1}{\lambda_{i}}$.
On voit que si un des élements est nul, donc si on a de la colinéarité la variance liée à ce paramètre diverge (dans la direction propre associée) et fausse le modèle.
De plus la variance estimée des erreurs dépend de n et p.
$$
\hat{\sigma}^2
=
\frac{1}{n - p}
\left\lVert
y - X\hat{\beta}
\right\rVert^2
$$

Si p>n, $X^{\top} X$ n'est plus inversible et l'estimateur n'est plus unique. Si p=n, donc on a plus de degré de liberté, on ne peut plus estimée $\hat{\sigma}^2$ qui devient indéfinie et par conséquent on ne peut plus estimer $\sigma$, le modèle devient non identifiable (le modèle n'aurait alors plus de sens). Enfin si la matrice $X^{\top}X$ est mal conditionné, notre modèle devient instable et donc pas utilisable.

C'est pourquoi, on va utiliser **Ridge** qui va nous permettre de palier à ces deux soucis:
réduire la dimensionalité et en même temps réduire les paramètres inutiles.

## Ridge

### Modèle brut

Ridge n'est pas gratuit, on va certes **baisser la variance** de nos estimateurs, mais on va aussi **augmenter leur biais**.

L'idée de la méthode Ridge, c'est d'ajouter une pénalisation $L^2$ associée à un hyper-paramètre $\lambda \ge 0$ ("shrinkage": on rétrécit les plages de valeurs que peuvent prendre les paramètres estimés) à notre problème d'optimisation: 

$$
\hat{\beta}_{\text{ridge}}
=
\arg\min_{\beta}
\left(
\| y - X\beta \|_2^2
+
\lambda \| \beta \|_2^2
\right)
$$ 

Cela permet de bien poser le problème peut importe la valeur de $p$ ou $n$ ou encore si on a des variables colinéaires.
On force un bon conditionnement, $X^{\top}X + \lambda I$ inversible et on borne la variance.

### Compréhension du modèle

Pour voir tout, ça on revient aux valeurs singulières de $X$.

On a comme solution unique du problème: $\hat{\beta} = (X^{\top} X+\lambda I)^{-1} X^{\top}y$
Cette fois l'estimateur est biaisé: 

$$
\mathbb{E}[\hat{\beta}]
=
(X^{\top} X+\lambda I)^{-1} X^{\top} X\, \beta
$$

et en se plaçant dans la base propre, $\mathbb{E}[\hat{\beta_i}] = \frac{\alpha_{i}^2}{\alpha_{i}^2 + \lambda} \beta_i $, donc un biais $\operatorname{Bias}(\hat{\beta}_i)= -\frac{\lambda}{\alpha_i^2 + \lambda}\,\beta_i$ (*)

On voit que le **biais est non nul** tant que $\lambda >0$ et de plus il est **borné** (en valeur absolue) par "le coefficient réel" avec la borne atteinte quand $\lambda \to +\infty$.
C'est logique, parce que si $\lambda = \infty$, on écrase les coefficients estimés qui valent 0.

De même avec la variance:

$$
\operatorname{Var}(\hat{\beta})
=
\sigma^2
(X^\top X + \lambda I)^{-1}
X^\top X
(X^\top X + \lambda I)^{-1}
$$

et en passant à la SVD:

$$
\operatorname{Var}(\hat{\beta}_i)
=
\sigma^2
\frac{\alpha_i^2}{(\alpha_i^2+\lambda)^2}
$$

On voit qu'on a clairement diminué la variance par rapport au modèle OSL et qu'elle est décroissante avec $\lambda$.
De plus, quand $\alpha_i = 0$, la variance et l'espérance sont nulles et alors le coefficient est nul avec une probabilité de 1.

Il faut en pratique prendre quelques précautions avec la Ridge:
- On normalise les $x_i$ qui deviennent alors centrés et réduits de telle sorte à ce que la pénalisation soit équitable et afin d'éviter que les observations à fortes variances aient trop d'influence (sans normalisation, elle serait beaucoup moins pénalisée, car $\alpha_i$ serait grand et on aurait un facteur de shrinkage (*) proche de 1).
- On centre alors la variable cible y pour ne pas pénaliser l'intercept. On enlève ainsi l'intercept, car les $x_i$ étant centrés, l'intercept vaut la moyenne de y. Ainsi, aucune pénalisation sera appliqué à l'intercept **En effet, celui-ci ne représente pas une association entre la variable cible et une variable explicative.**

### Prédiction

La question maintenant, c'est comment choisir $\lambda$.
